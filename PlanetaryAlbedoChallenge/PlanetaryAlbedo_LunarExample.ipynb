{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# plotting\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allow \"direct\" plotting pandas and xarray just in case\n",
    "import hvplot.pandas  # noqa\n",
    "import hvplot.xarray  # noqa\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "\n",
    "# setup plotting libs\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "%matplotlib inline\n",
    "\n",
    "# not necessary but why not\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# repeatability\n",
    "np.random.seed(123)\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and show raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "raw_data = {k: pd.read_csv('./Data/' + f, sep=',', header=None) for k, f in \n",
    "                [('albedo', 'Albedo.csv'), \n",
    "                 ('a', 'Element_A_Map.csv'), ('b', 'Element_B_Map.csv'),\n",
    "                 ('c', 'Element_C_Map.csv'), ('d', 'Element_D_Map.csv')]\n",
    "           }\n",
    "for r in raw_data.values():\n",
    "    r.index.name='y'\n",
    "    r.columns.name='x'\n",
    "    r.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.Dataset(raw_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.albedo.plot() # use hvplot to show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "plot_image = partial(hv.Image, kdims=['x', 'y'])\n",
    "\n",
    "def layout_vars(foo):\n",
    "    return lambda ds: hv.NdLayout({v: foo(ds[v]) for v in ds}, kdims=['variable']).cols(2)\n",
    "\n",
    "layout_vars(plot_image)(data).opts(opts.Image(width=len(data.x), height=len(data.y), colorbar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is albedo visibly related to elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridspace_vars(foo):\n",
    "    return lambda tidy: hv.GridSpace({\n",
    "        (a,b): foo(tidy, [a, b])\n",
    "        for a in tidy.columns\n",
    "        for b in tidy.columns\n",
    "    })        \n",
    "\n",
    "\n",
    "hv.render(\n",
    "    gridspace_vars(hv.Points) \\\n",
    "        (data.to_dataframe().query('albedo > 0')) \\\n",
    "        .opts(fig_size=256, backend='matplotlib'), # bokeh backend eats browser memory with too many client-side points\n",
    "    backend='matplotlib'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too many dots\n",
    "\n",
    "## Can we draw a better plot?\n",
    "\n",
    "### What would the profiling panda do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(data.to_dataframe().query('albedo > 0').set_index('albedo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HexTiles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_diag_vars(pair, solo):\n",
    "    return lambda tidy: hv.GridSpace({\n",
    "        (a,b): pair(tidy, [a, b]) if a!=b else solo(tidy, a)\n",
    "        for a in tidy.columns\n",
    "        for b in tidy.columns\n",
    "    })     \n",
    "\n",
    "def map_datasets(foo, kdims=None):\n",
    "    return lambda datasets: hv.HoloMap({\n",
    "        k: foo(ds)\n",
    "        for k, ds in datasets.items()\n",
    "    }, kdims=kdims).collate()\n",
    "\n",
    "def my_hex(ds):\n",
    "    return grid_diag_vars( \n",
    "        lambda ds, kdims: hv.Overlay([hv.HexTiles(ds, kdims)]),\n",
    "        lambda ds, a: hv.Overlay([hv.Histogram(np.histogram(ds[a].values, bins=20))])\n",
    "    )(ds).opts(\n",
    "        opts.HexTiles(colorbar=True, logz=True, gridsize=20),\n",
    "        opts.Overlay(width=180, height=180)\n",
    "    )\n",
    "\n",
    "my_hex(data.to_dataframe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The link is there, but looks pretty hard to train on\n",
    "## Is the data just noisy?\n",
    "http://scipy-lectures.org/packages/scikit-image/auto_examples/plot_filter_coins.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "from skimage import restoration\n",
    "from toolz import valmap\n",
    "\n",
    "smoothings = {\n",
    "    'none': data,\n",
    "    'gauss': data.map(lambda x: filters.gaussian(x, sigma=1.)),\n",
    "    'chambolle': data.map(lambda x: restoration.denoise_tv_chambolle(x, weight=2.)),\n",
    "    'median': data.map(lambda x: filters.median(x, np.ones((3, 3)) )),\n",
    "}\n",
    "\n",
    "map_datasets(layout_vars(plot_image), 'denoising')(\n",
    "    valmap(lambda ds: ds[{'x': slice(400, 450), 'y': slice(100, 150)}],\n",
    "           smoothings)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_datasets(my_hex, 'denoising')(valmap(xr.Dataset.to_dataframe, smoothings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does not look better\n",
    "- If anything, we need to reversee the projection, find original resolution for every variable, then denoise...\n",
    "- Maybe we can't just relate albedo to elements without context?\n",
    "  - Get context with a small convolution!\n",
    "- Or maybe it will work well enough?\n",
    "\n",
    "## Let's prepare a test split\n",
    "### The simplest way to manage data is to finish the cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patch size\n",
    "xx, yy = data.x[(data.albedo==0).any('y')], data.y[(data.albedo==0).any('x')]\n",
    "[int(x) for x in (xx.min(), xx.max(), yy.min(), yy.max(), data.x.max(), data.y.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_min, vx_max, vy_min, vy_max, x_max, y_max = [300, 430, 140, 270, 720, 360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[{'x': slice(vx_min, vx_max), 'y': slice(vy_min, vy_max)}]\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = data.roll(roll_coords=True, y=-vy_min)[{'x': slice(vx_min, vx_max), 'y': slice(vy_max - vy_min, None)}].roll(roll_coords=True, y=vy_min)\n",
    "test_data = data.where(\n",
    "    np.logical_and(\n",
    "        np.logical_and(data.x >= vx_min, data.x < vx_max),\n",
    "        np.logical_or(data.y >= vy_max, data.y < vy_min)\n",
    "    )\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = data.roll(roll_coords=True, x=-vx_min)[{'x': slice(vx_max-vx_min, None)}].roll(roll_coords=True, x=vx_min)\n",
    "train_data = data.where(\n",
    "    np.logical_or(data.x < vx_min, data.x >= vx_max)\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_datasets(foo, kdims=None):\n",
    "    return lambda datasets: hv.NdLayout({\n",
    "        k: foo(ds)\n",
    "        for k, ds in datasets.items()\n",
    "    }, kdims=kdims)\n",
    "    \n",
    "\n",
    "def map_variables(foo):\n",
    "    return lambda ds: hv.HoloMap({\n",
    "        k: foo(ds[k])\n",
    "        for k in ds\n",
    "    }, kdims='variables')\n",
    "\n",
    "\n",
    "layout_datasets(map_variables(plot_image), 'dataset')({\n",
    "    'full': data,\n",
    "    'train': train_data,\n",
    "    'test': test_data,\n",
    "    'validation': validation_data,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna('x', 'all')\n",
    "test_data = test_data.dropna('x', 'all').dropna('y', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_datasets(my_hex, 'dataset')(valmap(xr.Dataset.to_dataframe, {\n",
    "    'full': data,\n",
    "    'train': train_data,\n",
    "    'test': test_data,\n",
    "    # 'validation': validation_data, # all-zeros break hex?\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistically uneven, but so is the validation rectangle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make trivial linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_models = {k: LinearRegression() for k in 'abcd'}\n",
    "linear_data = data.copy()\n",
    "for k, m in linear_models.items():\n",
    "    m.fit(train_data.albedo.data.reshape(-1, 1), train_data[k].data.reshape(-1, 1))\n",
    "    prediction = m.predict(linear_data.albedo.data.reshape(-1, 1))\n",
    "    linear_data[k] = (('y', 'x'), prediction.reshape(linear_data[k].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model is linear!\n",
    "hv.Layout([hv.HexTiles(data.to_dataframe(), ['albedo', element]) * hv.Points(linear_data.to_dataframe().sample(100), ['albedo', element]) for element in 'abcd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_datasets(layout_vars(plot_image), 'model')\\\n",
    "({'original': data, 'linear': linear_data})\\\n",
    ".opts(opts.Image(width=len(data.x), height=len(data.y), colorbar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_score = pd.DataFrame([\n",
    "    {\n",
    "        'score': mean_squared_error(\n",
    "            ds[el].data.reshape(-1, 1),\n",
    "            linear_models[el].predict(ds.albedo.data.reshape(-1, 1))\n",
    "        ),\n",
    "        'dataset': ds_k,\n",
    "        'element': el,\n",
    "        'model': 'linear'\n",
    "    }\n",
    "    for ds_k, ds in [('test', test_data), ('train', train_data)]\n",
    "    for el in 'abcd'\n",
    "])\n",
    "linear_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.groupby(['dataset', 'model']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- worse than the 422.51792 of the original example solution\n",
    "  - because of the less random train/test split\n",
    "  - but maybe *more* representative of the expected validation result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trivial 1-d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_models = {k: XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, seed=13, n_jobs=-1) for k in 'abcd'}\n",
    "xgb_data = data.copy()\n",
    "for k, m in xgb_models.items():\n",
    "    m.fit(train_data.albedo.data.reshape(-1, 1), train_data[k].data.reshape(-1))\n",
    "    prediction = m.predict(xgb_data.albedo.data.reshape(-1, 1))\n",
    "    xgb_data[k] = (('y', 'x'), prediction.reshape(xgb_data[k].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Layout([hv.HexTiles(data.to_dataframe(), ['albedo', element]) * hv.Points(xgb_data.to_dataframe().sample(1000), ['albedo', element]) for element in 'abcd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_score = pd.DataFrame([\n",
    "    {\n",
    "        'score': mean_squared_error(\n",
    "            ds[el].data.reshape(-1, 1),\n",
    "            xgb_models[el].predict(ds.albedo.data.reshape(-1, 1))\n",
    "        ),\n",
    "        'dataset': ds_k,\n",
    "        'element': el,\n",
    "        'model': 'xgb'\n",
    "    }\n",
    "    for ds_k, ds in [('test', test_data), ('train', train_data)]\n",
    "    for el in 'abcd'\n",
    "])\n",
    "xgb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_datasets(layout_vars(plot_image), 'model')\\\n",
    "({'original': data, 'xgb': xgb_data})\\\n",
    ".opts(opts.Image(width=len(data.x), height=len(data.y), colorbar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "def create_model():\n",
    "    inputs = keras.Input(shape=(None, None, 1))\n",
    "    conv1 = keras.layers.Conv2D(filters=32, kernel_size=3, activation='elu')\n",
    "    conv2 = keras.layers.Conv2D(filters=64, kernel_size=3, activation='elu')\n",
    "    conv3 = keras.layers.Conv2D(filters=128, kernel_size=3, activation='elu')\n",
    "    conv4 = keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu')\n",
    "    model = keras.Model(inputs, conv4(conv3(conv2(conv1(inputs)))))\n",
    "    model.compile(loss='MSE', optimizer='adam', metrics=['MSE'])\n",
    "    return model\n",
    "\n",
    "model_conv = keras.wrappers.scikit_learn.KerasRegressor(build_fn=create_model, epochs=512, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_conv(ds, pad=4):\n",
    "    X = ds.albedo.transpose().data\n",
    "    X = np.pad(X, [(0,), (pad,)], 'wrap')\n",
    "    X = np.pad(X, [(pad,), (0,)], 'edge')[None, ..., None]\n",
    "    Y = ds[list('abcd')].to_array().transpose().data[None, ...]\n",
    "    return X, Y\n",
    "print([x.shape for x in prep_conv(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_conv.fit(*prep_conv(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history['MSE']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_data = data.copy()\n",
    "X, _ = prep_conv(data)\n",
    "pred_conv = model_conv.predict(X)\n",
    "print(pred_conv.shape)\n",
    "for i, k in enumerate('abcd'):\n",
    "    prediction = pred_conv[..., i].transpose()\n",
    "    conv_data[k] = (('y', 'x'), prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model_conv.predict(X_test)\n",
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_score = pd.DataFrame([\n",
    "    {\n",
    "        'score': mean_squared_error(\n",
    "            prep_conv(ds)[1][0, ..., i],\n",
    "            model_conv.predict(prep_conv(ds)[0])[..., i]\n",
    "        ),\n",
    "        'dataset': ds_k,\n",
    "        'element': el,\n",
    "        'model': 'conv'\n",
    "    }\n",
    "    for ds_k, ds in [('test', test_data), ('train', train_data)]\n",
    "    for i, el in enumerate('abcd')\n",
    "])\n",
    "conv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Layout([hv.HexTiles(data.to_dataframe(), ['albedo', element]) * hv.Points(conv_data.to_dataframe().sample(1000), ['albedo', element]) for element in 'abcd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_datasets(layout_vars(plot_image), 'model')\\\n",
    "({'original': data, 'conv': conv_data})\\\n",
    ".opts(opts.Image(width=len(data.x), height=len(data.y), colorbar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables\n",
    "\n",
    "* Google Colab Jupyter Notebook showing your solution along with the final model score More details regarding the format of the notebook can be found in the sample Google Colab notebook provided for this challenge.  \n",
    "* A txt file for each element containing your predictions on the test data. Format should be: x_coordinate, y_coordinate, predicted_value. Put name of element in file. An example is provided.\n",
    "* The final trained model including the model architecture and the trained weights (For example: HDF5 file, .pb file, .pt, .sav file, etc.). You are free to choose Machine Learning Framework of your choice.\n",
    "* Example submissions can be found https://drive.google.com/drive/folders/1EsqNLc5DzCsaJuvSTYF85gMS5PTVell4?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models\n",
    "Other formats can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(element_A_model, open('./element_A_model.sav', 'wb'))\n",
    "pickle.dump(element_B_model, open('./element_B_model.sav', 'wb'))\n",
    "pickle.dump(element_C_model, open('./element_C_model.sav', 'wb'))\n",
    "pickle.dump(element_D_model, open('./element_D_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map that only looks at test data (train pixels = -1e6 to ensure no overlap with predictions)\n",
    "reconstructed_A_predictions = np.zeros(len(element_B)) - 1e6\n",
    "reconstructed_B_predictions = np.zeros(len(element_B)) - 1e6\n",
    "reconstructed_C_predictions = np.zeros(len(element_B)) - 1e6\n",
    "reconstructed_D_predictions = np.zeros(len(element_B)) - 1e6\n",
    "\n",
    "# Fill in predicted values\n",
    "for (i, index) in enumerate(test_indices):\n",
    "    reconstructed_A_predictions[index] = element_A_pred[i][0]\n",
    "    reconstructed_B_predictions[index] = element_B_pred[i][0]\n",
    "    reconstructed_C_predictions[index] = element_C_pred[i][0]\n",
    "    reconstructed_D_predictions[index] = element_D_pred[i][0]\n",
    "        \n",
    "# Reshape into images\n",
    "reconstructed_A_predictions = reconstructed_A_predictions.reshape(data_shape)\n",
    "reconstructed_B_predictions = reconstructed_B_predictions.reshape(data_shape)\n",
    "reconstructed_C_predictions = reconstructed_C_predictions.reshape(data_shape)\n",
    "reconstructed_D_predictions = reconstructed_D_predictions.reshape(data_shape)\n",
    "\n",
    "# Get coordinates of pixel\n",
    "xs, ys = [], []\n",
    "A_values, B_values, C_values, D_values = [], [], [], []\n",
    "\n",
    "for i in range(len(reconstructed_A_predictions)):\n",
    "    for j in range(len(reconstructed_A_predictions[i])):\n",
    "        if reconstructed_A_predictions[i][j] == -1e6:\n",
    "            continue\n",
    "        xs.append(i)\n",
    "        ys.append(j)\n",
    "        A_values.append(reconstructed_A_predictions[i][j])\n",
    "        B_values.append(reconstructed_B_predictions[i][j])\n",
    "        C_values.append(reconstructed_C_predictions[i][j])\n",
    "        D_values.append(reconstructed_D_predictions[i][j])\n",
    "\n",
    "        \n",
    "# Save\n",
    "with open('./element_A_predictions.txt', 'w') as f:\n",
    "    for i in range(len(xs)):\n",
    "        this_str = '%i, %i, %f \\n' % (xs[i], ys[i], A_values[i])\n",
    "        f.write(this_str)\n",
    "        \n",
    "with open('./element_B_predictions.txt', 'w') as f:\n",
    "    for i in range(len(xs)):\n",
    "        this_str = '%i, %i, %f \\n' % (xs[i], ys[i], A_values[i])\n",
    "        f.write(this_str)\n",
    "        \n",
    "with open('./element_C_predictions.txt', 'w') as f:\n",
    "    for i in range(len(xs)):\n",
    "        this_str = '%i, %i, %f \\n' % (xs[i], ys[i], A_values[i])\n",
    "        f.write(this_str)\n",
    "        \n",
    "with open('./element_D_predictions.txt', 'w') as f:\n",
    "    for i in range(len(xs)):\n",
    "        this_str = '%i, %i, %f \\n' % (xs[i], ys[i], A_values[i])\n",
    "        f.write(this_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Diagnostic Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct map using predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire map\n",
    "reconstructed_element_A = element_A.copy()\n",
    "reconstructed_element_B = element_B.copy()\n",
    "reconstructed_element_C = element_C.copy()\n",
    "reconstructed_element_D = element_D.copy()\n",
    "\n",
    "for (i, index) in enumerate(test_indices):\n",
    "    \n",
    "    # If the predicted pixel is unmasked, put it in the new map\n",
    "    if index in flat_unmasked_indices[0]:\n",
    "        reconstructed_element_A[index] = element_A_pred[i][0]\n",
    "        reconstructed_element_B[index] = element_B_pred[i][0]\n",
    "        reconstructed_element_C[index] = element_C_pred[i][0]\n",
    "        reconstructed_element_D[index] = element_D_pred[i][0]\n",
    "\n",
    "    else:\n",
    "        reconstructed_element_A[index] = 0\n",
    "        reconstructed_element_B[index] = 0\n",
    "        reconstructed_element_C[index] = 0\n",
    "        reconstructed_element_D[index] = 0 \n",
    "        \n",
    "# Make withheld region\n",
    "reconstructed_element_A = np.ma.MaskedArray(reconstructed_element_A, flat_mask)\n",
    "reconstructed_element_B = np.ma.MaskedArray(reconstructed_element_B, flat_mask)\n",
    "reconstructed_element_C = np.ma.MaskedArray(reconstructed_element_C, flat_mask)\n",
    "reconstructed_element_D = np.ma.MaskedArray(reconstructed_element_D, flat_mask)\n",
    "\n",
    "# Reshape into image\n",
    "reconstructed_element_A = reconstructed_element_A.reshape(data_shape)\n",
    "reconstructed_element_B = reconstructed_element_B.reshape(data_shape)\n",
    "reconstructed_element_C = reconstructed_element_C.reshape(data_shape)\n",
    "reconstructed_element_D = reconstructed_element_D.reshape(data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot reconstructed map (including training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(26, 14))\n",
    "\n",
    "im = axs[0, 0].imshow(reconstructed_element_A)\n",
    "axs[0, 0].set_title('Reconstructed Element A')\n",
    "# Set Colorbars\n",
    "divider = make_axes_locatable(axs[0, 0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "im = axs[0, 1].imshow(reconstructed_element_B)\n",
    "axs[0, 1].set_title('Reconstructed Element B')\n",
    "divider = make_axes_locatable(axs[0, 1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "im = axs[1, 0].imshow(reconstructed_element_C)\n",
    "axs[1, 0].set_title('Reconstructed Element C')\n",
    "divider = make_axes_locatable(axs[1, 0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "im = axs[1, 1].imshow(reconstructed_element_D)\n",
    "axs[1, 1].set_title('Reconstructed Element D')\n",
    "divider = make_axes_locatable(axs[1, 1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot difference between predicted and actual maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between prediction and true\n",
    "difference_A = reconstructed_element_A - np.ma.MaskedArray(element_A_data, flat_mask)\n",
    "difference_B = reconstructed_element_B - np.ma.MaskedArray(element_B_data, flat_mask)\n",
    "difference_C = reconstructed_element_C - np.ma.MaskedArray(element_C_data, flat_mask)\n",
    "difference_D = reconstructed_element_D - np.ma.MaskedArray(element_D_data, flat_mask)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(26, 14))\n",
    "\n",
    "im = axs[0, 0].imshow(difference_A, cmap='RdBu_r')\n",
    "axs[0, 0].set_title('Predicted Element A - Element A')\n",
    "# Set Colorbars\n",
    "divider = make_axes_locatable(axs[0, 0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "im = axs[0, 1].imshow(difference_B, cmap='RdBu_r')\n",
    "axs[0, 1].set_title('Predicted Element B - Element B')\n",
    "divider = make_axes_locatable(axs[0, 1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "im = axs[1, 0].imshow(difference_C, cmap='RdBu_r')\n",
    "axs[1, 0].set_title('Predicted Element C - Element C')\n",
    "divider = make_axes_locatable(axs[1, 0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "im = axs[1, 1].imshow(difference_D, cmap='RdBu_r')\n",
    "axs[1, 1].set_title('Predicted Element D - Element D')\n",
    "divider = make_axes_locatable(axs[1, 1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
